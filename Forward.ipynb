{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reloading modified files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from modules import Network, LinearLayer, Sigmoid, ReLU, MSE, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement the functionality for some of the basic building blocks of artificial neural networks. You will create a network consisting of several layers each of which implements the `forward` function inherited from the base class `Module`. A skeleton for your implementation is provided in `modules.py`. Work through this notebook to validate your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the lecture, the notion of a *layer* is not well-defined and we may even regard a whole network as a layer predicting a desired output from input data. Therefore the class ```Network``` will be a subclass of ```Module```. Each subclass of ```Module``` has to provide a ```forward``` function mapping input to output. An instance of ```Network``` will itself store several layers of class ```Module```. Calling ```forward``` is supposed to sequentially execute ```forward``` on the layers stored in the network. Follow the comments below and complete the implementation in ```modules.py```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section introduces a selection of common layers used in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear layer performs an affine-linear transformation mapping input $x$ to $Wx + b$. Implement the class `LinearLayer` and test you implementation by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.ones((3, 4))\n",
    "b = np.arange(1, 4)\n",
    "\n",
    "linear = LinearLayer(W, b)\n",
    "x = np.ones(4)\n",
    "\n",
    "assert np.abs(np.max(linear.forward(x) - np.array([5, 6, 7]))) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid activation maps input $x$ to ${e^x} / (1 + e^x)$. Complete the forward pass of the class `Sigmoid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Sigmoid().forward(np.array([0, -1, 10]))\n",
    "out_expected = np.array([0.5, 0.2689414, 1.0])\n",
    "assert np.abs(np.max(out - out_expected)) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Re**ctified __L__inear **U**nits are the most common activations in use. In their forward pass non-negative input values are left unchanged and negative values are set to $0$, i.e. input $x$ is mapped to $\\max(x, 0)$, where the maximum is taken element-wise. \n",
    "\n",
    "Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ReLU().forward(np.array([-3.14, 0, 1, 10]))\n",
    "out_expected = np.array([0., 0., 1., 10])\n",
    "assert np.abs(np.max(out - out_expected)) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network is supposed to predict a desired output from input data. The quality of the prediction is assessed by *loss functions* comparing the predicted output with the target or ground truth. In our implementation a loss is a subclass of ```Module```. It therefore also features a ```forward``` function. All loss functions will take the output of a network and and a corresponding target vector as input for their forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE loss has already been discussed in the context of linear regression. Implement the `forward` function calculating the mean squared difference of prediction and target.\n",
    "\n",
    "Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = MSE().forward(np.array([0., 1., 2., 1.5]), np.array([0., 1., 1., -1.]))\n",
    "out_expected = 7.25/4\n",
    "assert np.abs(np.max(out - out_expected)) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many problem instances in the field of machine learning are formulated as *classification* tasks. Given some input $x$ we want to predict a discrete class label $l\\in\\{1, \\ldots, L\\}$. In order to train neural networks, the forward pass has to be differentiable. As the prediction of discrete values is not differentiable, we rather predict a vector in $\\mathbb{R}^L$ representing for each label the probability to be correct. We can actually transform this vector into a valid probability distribution using the softmax function (https://en.wikipedia.org/wiki/Softmax_function)\n",
    "$$\n",
    "\\sigma \\, \\colon \\, \\mathbb{R}^L \\to \\left\\{ \\sigma \\in \\mathbb{R}^L \\, \\middle| \\, \\sigma_i > 0, \\sum_{i=1}^L \\sigma_i = 1 \\right\\}, \\, \\sigma_j ( z ) = \\frac{e^{z_j}}{\\sum_{i=1}^L e^{z_i}} \\text{ for $j \\in \\left\\{ 1, \\ldots, L \\right\\}$}.\n",
    "$$\n",
    "\n",
    "This enables us to define a proper loss function by taking the negative logarithm of the predicted probability of the target label $l$, i.e. $\\ell (x, l) = -\\log (\\sigma_l (x))$. Implement the cross entropy loss, where $x$ is the prediction of our network and $l$ is the target label.\n",
    "\n",
    "Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = CrossEntropyLoss().forward(np.array([-3.14, 0, 1, 10]), 0)\n",
    "out_expected = 13.1401\n",
    "assert np.abs(np.max(out - out_expected)) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers in a network [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step, implement the class `Network` following the instructions in `modules.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.ones((3, 4))\n",
    "b1 = np.linspace(1, 3, 3)\n",
    "linear1 = LinearLayer(W1, b1)\n",
    "\n",
    "W2 = np.ones((1, 3))\n",
    "b2 = np.ones((1))\n",
    "linear2 = LinearLayer(W2, b2)\n",
    "\n",
    "relu = ReLU()\n",
    "\n",
    "net = Network([linear1, relu])\n",
    "\n",
    "net.add_layer(linear2)\n",
    "\n",
    "assert np.abs(np.max(net.forward(np.array([-3.14, 0, 1, 10])) - 30.58)) < 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
